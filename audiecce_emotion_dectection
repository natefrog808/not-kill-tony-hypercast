import * as tf from '@tensorflow/tfjs';
import { AudioAnalyzer } from './audio-analyzer';
import { VideoAnalyzer } from './video-analyzer';
import { ChatAnalyzer } from './chat-analyzer';

interface EmotionData {
  type: EmotionType;
  intensity: number;
  timestamp: number;
  source: 'audio' | 'video' | 'chat';
  confidence: number;
}

type EmotionType = 
  | 'LAUGHTER' 
  | 'APPLAUSE' 
  | 'SILENCE' 
  | 'DISAPPROVAL' 
  | 'EXCITEMENT';

interface AudienceMember {
  id: string;
  emotions: EmotionData[];
  engagementScore: number;
  location: { x: number; y: number };
}

class EmotionDetectionSystem {
  private audioAnalyzer: AudioAnalyzer;
  private videoAnalyzer: VideoAnalyzer;
  private chatAnalyzer: ChatAnalyzer;
  private emotionModel: tf.LayersModel;
  private audiences: Map<string, AudienceMember>;
  private realtimeMetrics: AudienceMetrics;

  constructor() {
    this.audioAnalyzer = new AudioAnalyzer();
    this.videoAnalyzer = new VideoAnalyzer();
    this.chatAnalyzer = new ChatAnalyzer();
    this.audiences = new Map();
    this.realtimeMetrics = new AudienceMetrics();
    this.initializeEmotionModel();
  }

  private async initializeEmotionModel() {
    this.emotionModel = await tf.loadLayersModel('emotion_detection_model');
  }

  async processAudioStream(audioStream: MediaStream): Promise<EmotionData[]> {
    const audioFeatures = await this.audioAnalyzer.extractFeatures(audioStream);
    return this.detectAudioEmotions(audioFeatures);
  }

  async processVideoStream(videoStream: MediaStream): Promise<EmotionData[]> {
    const frameFeatures = await this.videoAnalyzer.extractFeatures(videoStream);
    return this.detectVideoEmotions(frameFeatures);
  }

  async processChatMessages(messages: ChatMessage[]): Promise<EmotionData[]> {
    return this.chatAnalyzer.analyzeEmotions(messages);
  }

  private async detectAudioEmotions(audioFeatures: Float32Array): Promise<EmotionData[]> {
    const predictions = await this.emotionModel.predict(
      tf.tensor2d([audioFeatures])
    );

    return this.processEmotionPredictions(predictions, 'audio');
  }

  private async detectVideoEmotions(frameFeatures: Float32Array): Promise<EmotionData[]> {
    const predictions = await this.emotionModel.predict(
      tf.tensor2d([frameFeatures])
    );

    return this.processEmotionPredictions(predictions, 'video');
  }

  private processEmotionPredictions(
    predictions: tf.Tensor,
    source: 'audio' | 'video'
  ): EmotionData[] {
    const emotionData: EmotionData[] = [];
    const predictionArray = predictions.arraySync() as number[][];

    predictionArray[0].forEach((confidence, index) => {
      if (confidence > 0.5) {
        emotionData.push({
          type: this.mapIndexToEmotion(index),
          intensity: confidence,
          timestamp: Date.now(),
          source,
          confidence
        });
      }
    });

    return emotionData;
  }

  private mapIndexToEmotion(index: number): EmotionType {
    const emotionMap: EmotionType[] = [
      'LAUGHTER',
      'APPLAUSE',
      'SILENCE',
      'DISAPPROVAL',
      'EXCITEMENT'
    ];
    return emotionMap[index];
  }
}

class AudienceMetrics {
  private metrics: {
    overallEngagement: number;
    dominantEmotion: EmotionType;
    emotionIntensities: Map<EmotionType, number>;
    recentTrend: 'rising' | 'falling' | 'stable';
    audienceSize: number;
  };

  private readonly TREND_WINDOW = 10; // seconds
  private recentEmotions: EmotionData[] = [];

  constructor() {
    this.metrics = {
      overallEngagement: 0,
      dominantEmotion: 'SILENCE',
      emotionIntensities: new Map(),
      recentTrend: 'stable',
      audienceSize: 0
    };
  }

  updateMetrics(newEmotions: EmotionData[]): void {
    this.addToRecentEmotions(newEmotions);
    this.calculateEngagement();
    this.calculateDominantEmotion();
    this.calculateTrend();
  }

  private addToRecentEmotions(emotions: EmotionData[]): void {
    const now = Date.now();
    this.recentEmotions = [
      ...this.recentEmotions,
      ...emotions
    ].filter(e => now - e.timestamp < this.TREND_WINDOW * 1000);
  }

  private calculateEngagement(): void {
    if (this.recentEmotions.length === 0) {
      this.metrics.overallEngagement = 0;
      return;
    }

    const engagementScores = this.recentEmotions.map(emotion => {
      switch (emotion.type) {
        case 'LAUGHTER':
          return emotion.intensity * 1.5;
        case 'APPLAUSE':
          return emotion.intensity * 1.3;
        case 'EXCITEMENT':
          return emotion.intensity * 1.2;
        case 'SILENCE':
          return emotion.intensity * 0.5;
        case 'DISAPPROVAL':
          return emotion.intensity * 0.7;
        default:
          return emotion.intensity;
      }
    });

    this.metrics.overallEngagement = 
      engagementScores.reduce((a, b) => a + b, 0) / engagementScores.length;
  }

  private calculateDominantEmotion(): void {
    const emotionCounts = new Map<EmotionType, number>();
    
    this.recentEmotions.forEach(emotion => {
      const current = emotionCounts.get(emotion.type) || 0;
      emotionCounts.set(emotion.type, current + emotion.intensity);
    });

    let maxCount = 0;
    emotionCounts.forEach((count, emotion) => {
      if (count > maxCount) {
        maxCount = count;
        this.metrics.dominantEmotion = emotion;
      }
    });

    this.metrics.emotionIntensities = emotionCounts;
  }

  private calculateTrend(): void {
    const windowSize = Math.floor(this.TREND_WINDOW / 2);
    const midpoint = Math.floor(this.recentEmotions.length / 2);
    
    const firstHalf = this.recentEmotions
      .slice(0, midpoint)
      .reduce((sum, e) => sum + e.intensity, 0);
    
    const secondHalf = this.recentEmotions
      .slice(midpoint)
      .reduce((sum, e) => sum + e.intensity, 0);

    const difference = secondHalf - firstHalf;
    
    if (difference > windowSize * 0.1) {
      this.metrics.recentTrend = 'rising';
    } else if (difference < -windowSize * 0.1) {
      this.metrics.recentTrend = 'falling';
    } else {
      this.metrics.recentTrend = 'stable';
    }
  }

  getMetrics() {
    return this.metrics;
  }
}

class PerformanceFeedbackSystem {
  private emotionDetection: EmotionDetectionSystem;
  private audienceMetrics: AudienceMetrics;
  private currentPerformance?: {
    performerId: string;
    startTime: number;
    emotions: EmotionData[];
  };

  constructor() {
    this.emotionDetection = new EmotionDetectionSystem();
    this.audienceMetrics = new AudienceMetrics();
  }

  async startPerformanceTracking(performerId: string): Promise<void> {
    this.currentPerformance = {
      performerId,
      startTime: Date.now(),
      emotions: []
    };
  }

  async processFrame(
    audioStream: MediaStream,
    videoStream: MediaStream,
    chatMessages: ChatMessage[]
  ): Promise<void> {
    if (!this.currentPerformance) return;

    const [audioEmotions, videoEmotions, chatEmotions] = await Promise.all([
      this.emotionDetection.processAudioStream(audioStream),
      this.emotionDetection.processVideoStream(videoStream),
      this.emotionDetection.processChatMessages(chatMessages)
    ]);

    const allEmotions = [...audioEmotions, ...videoEmotions, ...chatEmotions];
    this.currentPerformance.emotions.push(...allEmotions);
    this.audienceMetrics.updateMetrics(allEmotions);
  }

  getRealtimeMetrics() {
    return this.audienceMetrics.getMetrics();
  }

  shouldAdjustTiming(): boolean {
    const metrics = this.audienceMetrics.getMetrics();
    return (
      metrics.dominantEmotion === 'LAUGHTER' &&
      metrics.emotionIntensities.get('LAUGHTER')! > 0.7
    );
  }
}

export { 
  EmotionDetectionSystem, 
  AudienceMetrics, 
  PerformanceFeedbackSystem 
};
